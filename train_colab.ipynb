{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAbLbgyzC6C_",
        "outputId": "91f5f6ee-ad93-4fc6-fa33-62069ce315aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Instruction-Tuning'...\n",
            "remote: Enumerating objects: 200, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 200 (delta 6), reused 11 (delta 3), pack-reused 183 (from 1)\u001b[K\n",
            "Receiving objects: 100% (200/200), 57.35 KiB | 11.47 MiB/s, done.\n",
            "Resolving deltas: 100% (112/112), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/kogby/Instruction-Tuning.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rm -rf __MACOSX/"
      ],
      "metadata": {
        "id": "orUdMwIDn0Qh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd Instruction-Tuning/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-R5-Ywe4C8-V",
        "outputId": "944e30eb-c7ef-45a0-9ff6-1f9b4b6a3f60"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Instruction-Tuning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash download.sh\n",
        "\n",
        "# !pip install evaluate\n",
        "# !pip install datasets\n",
        "# !pip install -r requirements.txt\n",
        "# !pip install -r requirements_qlora.txt\n",
        "# !pip install transformers==4.45.1\n",
        "\n",
        "\n",
        "## Use the following only when needed\n",
        "# !pip install --upgrade transformers\n",
        "# !pip install torch torchvision --upgrade\n",
        "# !pip install transformers[torch]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Emrf-uDSDKQo",
        "outputId": "308ce171-ac02-48f1-cbb1-54df4839c969"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1WIOj9SUvM4tSGSMqrfpo7q8xgAjiTJ-o\n",
            "From (redirected): https://drive.google.com/uc?id=1WIOj9SUvM4tSGSMqrfpo7q8xgAjiTJ-o&confirm=t&uuid=d6d0dbdf-f329-442c-9592-dcf6fa98037c\n",
            "To: /content/Instruction-Tuning/adapter_checkpoint.zip\n",
            "100% 47.3M/47.3M [00:00<00:00, 84.5MB/s]\n",
            "Archive:  adapter_checkpoint.zip\n",
            "   creating: adapter_checkpoint/\n",
            "  inflating: adapter_checkpoint/Adapter Model.safetensors  \n",
            "  inflating: __MACOSX/adapter_checkpoint/._Adapter Model.safetensors  \n",
            "  inflating: adapter_checkpoint/adapter_config.json  \n",
            "  inflating: __MACOSX/adapter_checkpoint/._adapter_config.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git pull\n",
        "!bash eval.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unrtDEG46arP",
        "outputId": "c72fb583-edfe-4231-c81d-d8ddb7256f5d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
            "Loading checkpoint shards: 100% 3/3 [00:03<00:00,  1.04s/it]\n",
            "  0% 0/250 [00:00<?, ?it/s]Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
            "2024-11-08 15:32:16.062906: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-11-08 15:32:16.083316: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-08 15:32:16.102594: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-08 15:32:16.108051: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-08 15:32:16.122367: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-08 15:32:17.271114: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "100% 250/250 [02:49<00:00,  1.48it/s]\n",
            "Mean perplexity: 16.376318541049958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash train.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E98Edux8FCQa",
        "outputId": "615eacae-0d22-4cc2-b908-090234294be8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-08 08:03:46.196005: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-11-08 08:03:46.213905: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-08 08:03:46.235221: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-08 08:03:46.241801: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-08 08:03:46.257730: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-08 08:03:47.482043: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Namespace(model_name_or_path='zake7749/gemma-2-2b-it-chinese-kyara-dpo', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=None, source_max_len=1024, target_max_len=512, dataset='data/train.json', dataset_format=None, output_dir='./output', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, eval_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, torch_empty_cache_steps=None, learning_rate=0.0008, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=5000, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./output/runs/Nov08_08-03-50_e3334266db9e', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=20, save_total_limit=40, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name='./output', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False), deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, hub_always_push=False, gradient_checkpointing=True, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, eval_do_concat_batches=True, fp16_backend='auto', evaluation_strategy=None, push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, split_batches=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, eval_on_start=False, use_liger_kernel=False, eval_use_gather_object=False, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {\n",
            "  \"max_new_tokens\": 256\n",
            "}\n",
            ", cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16, lora_dropout=0.05, max_memory_MB=80000, distributed_state=Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            ", _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)\n",
            "Found a previous checkpoint at: ./output/checkpoint-250\n",
            "loading base model zake7749/gemma-2-2b-it-chinese-kyara-dpo...\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.41s/it]\n",
            "Loading adapters from checkpoint.\n",
            "loaded model\n",
            "Formatted dataset with input-output format.\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "trainable params: 6389760.0 || all params: 1614983424 || trainable: 0.395654834906838\n",
            "torch.float32 602845440 0.37328274150756857\n",
            "torch.uint8 1012137984 0.6267172584924314\n",
            "{'loss': 2.5737, 'grad_norm': 0.46429359912872314, 'learning_rate': 0.0008, 'epoch': 0.14}\n",
            "{'loss': 2.3224, 'grad_norm': 0.4645937979221344, 'learning_rate': 0.0008, 'epoch': 0.28}\n",
            "  0% 20/5000 [02:42<11:42:11,  8.46s/it]Saving PEFT checkpoint...\n",
            "{'loss': 2.3207, 'grad_norm': 0.3863697350025177, 'learning_rate': 0.0008, 'epoch': 0.43}\n",
            "{'loss': 2.288, 'grad_norm': 0.3396839499473572, 'learning_rate': 0.0008, 'epoch': 0.57}\n",
            "  1% 40/5000 [05:22<11:17:00,  8.19s/it]Saving PEFT checkpoint...\n",
            "{'loss': 2.1421, 'grad_norm': 0.43578219413757324, 'learning_rate': 0.0008, 'epoch': 0.71}\n",
            "{'loss': 2.2773, 'grad_norm': 0.3124344050884247, 'learning_rate': 0.0008, 'epoch': 0.85}\n",
            "  1% 60/5000 [08:00<10:50:45,  7.90s/it]Saving PEFT checkpoint...\n",
            "{'loss': 2.098, 'grad_norm': 0.39477747678756714, 'learning_rate': 0.0008, 'epoch': 1.0}\n",
            "{'loss': 2.1878, 'grad_norm': 0.3076893985271454, 'learning_rate': 0.0008, 'epoch': 1.14}\n",
            "  2% 80/5000 [10:38<10:42:36,  7.84s/it]Saving PEFT checkpoint...\n",
            "{'loss': 1.966, 'grad_norm': 0.33056506514549255, 'learning_rate': 0.0008, 'epoch': 1.28}\n",
            "{'loss': 2.0833, 'grad_norm': 0.3968285918235779, 'learning_rate': 0.0008, 'epoch': 1.42}\n",
            "  2% 100/5000 [13:15<10:15:36,  7.54s/it]Saving PEFT checkpoint...\n",
            "{'loss': 2.1059, 'grad_norm': 0.30633851885795593, 'learning_rate': 0.0008, 'epoch': 1.56}\n",
            "{'loss': 1.9351, 'grad_norm': 0.44026923179626465, 'learning_rate': 0.0008, 'epoch': 1.71}\n",
            "  2% 120/5000 [15:52<9:52:53,  7.29s/it]Saving PEFT checkpoint...\n",
            "{'loss': 2.1399, 'grad_norm': 0.32063838839530945, 'learning_rate': 0.0008, 'epoch': 1.85}\n",
            "  3% 137/5000 [18:06<9:49:57,  7.28s/it]Traceback (most recent call last):\n",
            "  File \"/content/Instruction-Tuning/qlora.py\", line 852, in <module>\n",
            "    train()\n",
            "  File \"/content/Instruction-Tuning/qlora.py\", line 814, in train\n",
            "    train_result = trainer.train()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2052, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2388, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3518, in training_step\n",
            "    self.accelerator.backward(loss, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 2196, in backward\n",
            "    loss.backward(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 581, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n",
            "    _engine_run_backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n",
            "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n",
            "  3% 137/5000 [18:13<10:47:00,  7.98s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash run.sh zake7749/gemma-2-2b-it-chinese-kyara-dpo adapter_checkpoint/ data/private_test.json prediction.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdWcnX2HNXz8",
        "outputId": "6af42c1f-e066-4194-d91c-10acb6a71865"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
            "Loading checkpoint shards: 100% 3/3 [00:03<00:00,  1.00s/it]\n",
            "The 'max_batch_size' argument of HybridCache is deprecated and will be removed in v4.46. Use the more precisely named 'batch_size' argument instead.\n",
            "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
            "2024-11-08 15:35:15.822305: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-11-08 15:35:15.838690: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-08 15:35:15.857795: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-08 15:35:15.863269: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-08 15:35:15.876796: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-08 15:35:17.040765: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hz2_MFggafv_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}